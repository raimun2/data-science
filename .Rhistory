siluetas <- rbind(siluetas, c(mean(silhouette(c, d)[,3]), paste0(m, "-k:",ks,"-fuzz:",fz)))
}
}
}
if(m == "gmm"){
for(ks in nks){
for(cv in cov){
c <- clusteriza(data_tsne, model = m, k = ks, cov = cv)
siluetas <- rbind(siluetas, c(mean(silhouette(c, d)[,3]), paste0(m, "-k:",ks,"-cov:",cv)))
}
}
}
if(m == "dbscan"){
for(ep in dists){
for(mp in 5*(1:9)){
c <- clusteriza(data_tsne, model = m, eps = ep, minpts = mp)
siluetas <- rbind(siluetas, c(mean(silhouette(c, d)[,3]), paste0(m, "-eps:",eps,"-minpts:",mp)))
}
}
}
}
# creamos vector vacio para almacenar siluetas
siluetas <- numeric(20)
# creamos vector vacio para almacenar siluetas
siluetas <- numeric(20)
for (k in 2:20){
# ejecutamos kmedias con k centroides
modelo <- kmeans(data_tsne, centers = k)
# cramos objeto con la silueta
temp <- silhouette(modelo$cluster, dist(data_tsne))
# almacenamos la silueta promedio del modelo
siluetas[k] <- mean(temp[,3])
}
tempDF <- data.frame(CS=siluetas, K=c(1:20))
# visualizamos
ggplot(tempDF, aes(x=K, y=CS)) +
geom_line() +
scale_x_continuous(breaks=c(1:20)) +
geom_vline(xintercept = which(tempDF$CS == max(tempDF$CS)), col = "red")
source("R/clusteriza.R")
# cargo librerias
pacman::p_load(tidyverse, Rtsne, mclust, e1071, cluster, flexclust, factoextra)
set.seed(42)
# cargo la data y aplico los mismos tratamientos que en el caso de DBScan
data_tsne  <- read.csv("data/video_games_sales.csv") %>%
mutate(User_Score = as.numeric(User_Score)) %>%
filter(!(is.na(Critic_Score) | is.na(User_Score))) %>%
select(Critic_Score, User_Score, User_Count, Global_Sales) %>%
unique() %>%
Rtsne() %>%
.$Y %>%
as.data.frame()
# exploramos graficamente la data
ggplot(data_tsne) +
geom_point(aes(V1,V2))
# antes de clusterizar, evaluamos el grado de clusterizacion natural de los datos de acuerdo
# al estadistico de hopkins, que esta implementado en la libreria factoextra.
#Calcula el hopkins statistic
(res <- get_clust_tendency(data_tsne, n = 30, graph = FALSE))
## Evaluacion de clusters
# ejecutamos un modelo para probar su clusterizacion
modelo_kmeans <- kmeans(data_tsne, 13)
# almacenamos los clusters en un vector que contiene el numero de cada cluster y el indice
clusters <- modelo_kmeans$cluster
# calculamos las distancias de los datos
distancias <- dist(data_tsne) %>% as.matrix()
View(distancias)
# generamos indices con la ubicacion de los clusters ordenados
clusters_i <-  sort(clusters, index.return=TRUE)
clusters_i$ix
#reordeno filas y columnas en base al cluster obtenido
distancias <- distancias[clusters_i$ix, clusters_i$ix]
rownames(distancias) <- c(1:nrow(data_tsne))
colnames(distancias) <- c(1:nrow(data_tsne))
# pero la matriz de distancias es muy grande para graficar
print(object.size(distancias), units = "Mb")
# la extraemos 1 de cada 10 filas y columnas
n <- 10
ids <- (1:floor(nrow(distancias)/n))*n
dist_reducida <- distancias[ids,ids]
# bajo considerablemente el tamaño
print(object.size(dist_reducida), units = "Mb")
# generamos la imagen de la matriz para la inspececion visual
image(dist_reducida)
# creo matriz llega de ceros
matriz_ideal <- matrix(0, nrow = nrow(data_tsne), ncol = nrow(data_tsne))
# creo matriz llega de ceros
matriz_ideal <- matrix(0, nrow = nrow(data_tsne), ncol = nrow(data_tsne))
# para cada cluster reemplazo con 1 en aquellas entidades que pertenezcan a el
for(k in unique(clusters_i$x)){
matriz_ideal[which(clusters_i$x==k), which(clusters_i$x==k)]  <- 1
}
# construyo matriz de disimilitud en base a la distancia ordenada
tempDist2 <- 1/(1+distancias)
? cor
# Calcula correlacion entre matriz de disimilitud y matriz optima
cor <- cor(matriz_ideal[upper.tri(matriz_ideal)],tempDist2[upper.tri(tempDist2)])
print(cor)
# creamos vector vacio para medir la cohesion en cada cluster
withinCluster <- numeric(length(unique(clusters)))
withinCluster
# creamos vector vacio para medir la cohesion en cada cluster
withinCluster <- numeric(length(unique(clusters)))
# para cada cluster
for (i in 1:length(withinCluster)){
# extraigo los puntos pertenecientes al cluster i
tempData <- data_tsne[which(clusters == i),]
# calculo la suma de distancias al cuadrado entre cada punto y el centroide
withinCluster[i] <- sum(dist2(tempData,colMeans(tempData))^2)
}
# calculo la suma total de cohesion para todos los clusters
cohesion <- sum(withinCluster)
#es equivalente a model$tot.withinss en k-means
print(c(cohesion, modelo_kmeans$tot.withinss))
centroide_total <- colMeans(data_tsne)
centroide_total
# creamos vector vacio para almacenar separacion en cada cluster
separation <-  numeric(length(unique(clusters)))
# para cada cluster
for (i in 1:length(separation)){
# extraigo los puntos pertenecientes al cluster i
tempData <- data_tsne[which(clusters==i),]
# calculo la separacion como la distancia promedio entre cada punto de un cluster y el resto
separation[i] <- nrow(tempData)*sum((meanData-colMeans(tempData))^2)
}
# creamos vector vacio para almacenar separacion en cada cluster
separation <-  numeric(length(unique(clusters)))
# para cada cluster
for (i in 1:length(separation)){
# extraigo los puntos pertenecientes al cluster i
tempData <- data_tsne[which(clusters==i),]
# calculo la separacion como la distancia promedio entre cada punto de un cluster y el resto
separation[i] <- nrow(tempData)*sum((separation-colMeans(tempData))^2)
}
(sum(separation))
centroide_total <- colMeans(data_tsne)
# creamos vector vacio para almacenar separacion en cada cluster
separation <-  numeric(length(unique(clusters)))
# para cada cluster
for (i in 1:length(separation)){
# extraigo los puntos pertenecientes al cluster i
tempData <- data_tsne[which(clusters==i),]
# calculo la separacion como la distancia promedio entre cada punto de un cluster y el resto
separation[i] <- nrow(tempData)*sum((centroide_total-colMeans(tempData))^2)
}
(sum(separation))
? silhouette
clusters
# el coeficiente recibe las etiquetas de cluster y la matriz de distancias de la data original
coefSil <- silhouette(clusters, dist(data_tsne))
summary(coefSil)
coefSil[,3]
#visualizamos el codigo de silueta de cada cluster
fviz_silhouette(coefSil) + coord_flip()
# el coeficiente recibe las etiquetas de cluster y la matriz de distancias de la data original
coefSil <- silhouette(clusters, dist(data_tsne))
#visualizamos el codigo de silueta de cada cluster
fviz_silhouette(coefSil) + coord_flip()
# creamos vector vacio para almacenar siluetas
siluetas <- numeric(20)
# creamos vector vacio para almacenar siluetas
siluetas <- numeric(20)
for (k in 2:20){
# ejecutamos kmedias con k centroides
modelo <- kmeans(data_tsne, centers = k)
# cramos objeto con la silueta
temp <- silhouette(modelo$cluster, dist(data_tsne))
# almacenamos la silueta promedio del modelo
siluetas[k] <- mean(temp[,3])
}
tempDF <- data.frame(CS=siluetas, K=c(1:20))
# visualizamos
ggplot(tempDF, aes(x=K, y=CS)) +
geom_line() +
scale_x_continuous(breaks=c(1:20)) +
geom_vline(xintercept = which(tempDF$CS == max(tempDF$CS)), col = "red")
source("R/clusteriza.R")
tuesdata <- tidytuesdayR::tt_load('2020-09-22')
library(munro)
pacman::p_load(tidymodels)
library(rlang)
library(tidymodels)
tidymodels_prefer()
pacman::p_load(tidymodels)
tidymodels_prefer()
data(Chicago)
n <- nrow(Chicago)
Chicago <- Chicago %>% select(ridership, Clark_Lake, Quincy_Wells)
Chicago_train <- Chicago[1:(n - 7), ]
Chicago_test <- Chicago[(n - 6):n, ]
knn_reg_spec <-
nearest_neighbor(neighbors = 5, weight_func = "triangular") %>%
# This model can be used for classification or regression, so set mode
set_mode("regression") %>%
set_engine("kknn")
knn_reg_spec
knn_reg_fit <- knn_reg_spec %>% fit(ridership ~ ., data = Chicago_train)
knn_reg_fit
install.packages("kknn")
knn_reg_fit <- knn_reg_spec %>% fit(ridership ~ ., data = Chicago_train)
knn_reg_fit
predict(knn_reg_fit, Chicago_test)
data(two_class_dat)
data_train <- two_class_dat[-(1:10), ]
data_test  <- two_class_dat[  1:10 , ]
knn_cls_spec <-
nearest_neighbor(neighbors = 11, weight_func = "triangular") %>%
# This model can be used for classification or regression, so set mode
set_mode("classification") %>%
set_engine("kknn")
knn_cls_spec
knn_cls_fit <- knn_cls_spec %>% fit(Class ~ ., data = data_train)
knn_cls_fit
bind_cols(
predict(knn_cls_fit, data_test),
predict(knn_cls_fit, data_test, type = "prob")
)
? tidymodels_prefer
View(Chicago)
# modelo prediccion
# cargamos la data
data(Chicago)
View(Chicago)
? Chicago
# seleccionamos variables relevantes
Chicago <- Chicago %>% select(ridership, Clark_Lake, Quincy_Wells)
Chicago_train <- Chicago[1:(n - 7), ]
n
1:(n - 7)
(n - 6):n
# cargo la data
data(two_class_dat)
? two_class_dat
View(two_class_dat)
# cargo librerias
pacman::p_load(tidymodels, kknn)
# modelo prediccion
# cargamos la data
data(Chicago)
View(Chicago)
# identificamos el numero de filas
n <- nrow(Chicago)
# seleccionamos variables relevantes
Chicago <- Chicago %>% select(ridership, Clark_Lake, Quincy_Wells)
# separo datos de entrenamiento y de prueba
Chicago_train <- Chicago[1:(n - 7), ]
Chicago_test <- Chicago[(n - 6):n, ]
View(Chicago_test)
# construyo el modelo
knn_reg_spec <-
nearest_neighbor(neighbors = 5, weight_func = "triangular") %>%
set_mode("regression") %>%
set_engine("kknn")
# veo el modelo
knn_reg_spec
# ajusto parametros del modelo
knn_reg_fit <- knn_reg_spec %>% fit(ridership ~ ., data = Chicago_train)
# veo prediccion
knn_reg_fit
# predigo valores para conjunto de prueba
predict(knn_reg_fit, Chicago_test)
# cargo la data
data(two_class_dat)
View(two_class_dat)
# separo datos de entrenamiento y de prueba
data_train <- two_class_dat[-(1:10), ]
data_test  <- two_class_dat[  1:10 , ]
knn_cls_spec <-
nearest_neighbor(neighbors = 11, weight_func = "triangular") %>%
set_mode("classification") %>%
set_engine("kknn")
knn_cls_spec
knn_cls_fit <- knn_cls_spec %>% fit(Class ~ ., data = data_train)
knn_cls_fit
bind_cols(
predict(knn_cls_fit, data_test),
predict(knn_cls_fit, data_test, type = "prob")
)
bind_cols(
predict(knn_cls_fit, data_test),
predict(knn_cls_fit, data_test)
)
bind_cols(
predict(knn_cls_fit, data_test),
predict(knn_cls_fit, data_test, type = "prob")
)
pacman::p_load(tidymodels, tidyverse, nycflights13)
set.seed(42)
# cargar y limpiar datos ----
flight_data <-
flights %>%
mutate(
# discretiza arr_delay y lo hace factor
arr_delay = ifelse(arr_delay >= 30, "late", "on_time"),
arr_delay = factor(arr_delay),
# obtengo solo la fecha a partir de fecha con hora
date = lubridate::as_date(time_hour)
) %>%
# combino con data de clima
inner_join(weather, by = c("origin", "time_hour")) %>%
# especifico las columnas de interes
select(dep_time, flight, origin, dest, air_time, distance,
carrier, date, arr_delay, time_hour) %>%
# excluyo datos faltates
na.omit() %>%
# transformo characteres en factores
mutate_if(is.character, as.factor)  %>%
# tomo una muestra de tamaño 10.000 para poder ejecutar los modelos
sample_n(10000)
## inspeccion de los datos
flight_data %>%
count(arr_delay) %>%
mutate(prop = n/sum(n))
glimpse(flight_data)
flight_data %>%
skimr::skim(dest, carrier)
-----------------------------------
# especificar receta ----
arr_delay ~ .
flights_rec <-
recipe(arr_delay ~ ., data = flight_data)
flight_data %>%
distinct(date) %>%
mutate(numeric_date = as.numeric(date))
# modificando fechas
flights_rec <-
recipe(arr_delay ~ ., data = flight_data) %>%
update_role(flight, time_hour, new_role = "ID") %>%
step_date(date, features = c("dow", "month")) %>%
step_holiday(date,
holidays = timeDate::listHolidays("US"),
keep_original_cols = FALSE)
View(flights_rec)
data_split <- initial_split(flight_data, prop = 3/4)
train_data <- training(data_split)
test_data  <- testing(data_split)
View(test_data)
flights_aug %>%
count(arr_delay) %>%
mutate(prop = n/sum(n))
flight_data %>%
skimr::skim(dest, carrier)
# quitar variables sin varianza
flights_rec <-
recipe(arr_delay ~ ., data = flight_data) %>%
update_role(flight, time_hour, new_role = "ID") %>%
step_date(date, features = c("dow", "month")) %>%
step_holiday(date,
holidays = timeDate::listHolidays("US"),
keep_original_cols = FALSE) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors())
knn_mod <- nearest_neighbor() %>%
set_mode("classification")
flights_wflow <-
workflow() %>%
add_model(knn_mod) %>%
add_recipe(flights_rec)
flights_wflow
data_split <- initial_split(flight_data, prop = 3/4)
train_data <- training(data_split)
test_data  <- testing(data_split)
train_data %>%
count(arr_delay) %>%
mutate(prop = n/sum(n))
data_split_strat <- initial_split(flight_data, prop = 3/4, strata = arr_delay)
train_data <- training(data_split_strat)
test_data  <- testing(data_split_strat)
train_data %>%
count(arr_delay) %>%
mutate(prop = n/sum(n))
train_data %>%
count(arr_delay) %>%
mutate(prop = n/sum(n))
flights_fit <-
flights_wflow %>%
fit(data = train_data)
data_post <- flight_data %>% flights_rec()
data_post <- flight_data %>% add_recipe(flights_rec)
flights_aug %>%
select(arr_delay, time_hour, flight, .pred_class, .pred_on_time)
pacman::p_load(tidymodels, tidyverse, nycflights13)
set.seed(42)
# cargar y limpiar datos ----
flight_data <-
flights %>%
mutate(
# discretiza arr_delay y lo hace factor
arr_delay = ifelse(arr_delay >= 30, "late", "on_time"),
arr_delay = factor(arr_delay),
# obtengo solo la fecha a partir de fecha con hora
date = lubridate::as_date(time_hour)
) %>%
# combino con data de clima
inner_join(weather, by = c("origin", "time_hour")) %>%
# especifico las columnas de interes
select(dep_time, flight, origin, dest, air_time, distance,
carrier, date, arr_delay, time_hour) %>%
# excluyo datos faltates
na.omit() %>%
# transformo characteres en factores
mutate_if(is.character, as.factor)  %>%
# tomo una muestra de tamaño 10.000 para poder ejecutar los modelos
sample_n(10000)
## inspeccion de los datos
flight_data %>%
count(arr_delay) %>%
mutate(prop = n/sum(n))
glimpse(flight_data)
-----------------------------------
# especificar receta ----
arr_delay ~ .
# receta simple
flights_rec <-
recipe(arr_delay ~ ., data = flight_data)
# receta ignorando algunas variables
flights_rec <-
recipe(arr_delay ~ ., data = flight_data) %>%
update_role(flight, time_hour, new_role = "ID")
# visualizar
summary(flights_rec)
flight_data %>%
distinct(date) %>%
mutate(numeric_date = as.numeric(date))
# modificando fechas
flights_rec <-
recipe(arr_delay ~ ., data = flight_data) %>%
update_role(flight, time_hour, new_role = "ID") %>%
step_date(date, features = c("dow", "month")) %>%
step_holiday(date,
holidays = timeDate::listHolidays("US"),
keep_original_cols = FALSE)
# convertir predictores nominales a dummys
flights_rec <-
recipe(arr_delay ~ ., data = flight_data) %>%
update_role(flight, time_hour, new_role = "ID") %>%
step_date(date, features = c("dow", "month")) %>%
step_holiday(date,
holidays = timeDate::listHolidays("US"),
keep_original_cols = FALSE) %>%
step_dummy(all_nominal_predictors())
# quitar variables sin varianza
flights_rec <-
recipe(arr_delay ~ ., data = flight_data) %>%
update_role(flight, time_hour, new_role = "ID") %>%
step_date(date, features = c("dow", "month")) %>%
step_holiday(date,
holidays = timeDate::listHolidays("US"),
keep_original_cols = FALSE) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors())
# especificar modelo ----
nearest_neighbor()
nearest_neighbor() %>%
set_mode("classification")
knn_mod <- nearest_neighbor() %>%
set_mode("classification")
# genera workflow ----
flights_wflow <-
workflow() %>%
add_model(knn_mod) %>%
add_recipe(flights_rec)
flights_wflow
# split data ----
data_split <- initial_split(flight_data, prop = 3/4)
train_data <- training(data_split)
test_data  <- testing(data_split)
train_data %>%
count(arr_delay) %>%
mutate(prop = n/sum(n))
data_split_strat <- initial_split(flight_data, prop = 3/4, strata = arr_delay)
train_data <- training(data_split_strat)
test_data  <- testing(data_split_strat)
train_data %>%
count(arr_delay) %>%
mutate(prop = n/sum(n))
# ajuste de modelos  ----
# ajuste con data entrenamiento
flights_fit <-
flights_wflow %>%
fit(data = train_data)
# prediccion con data test
predict(flights_fit, test_data)
# amplificacion data test con predicciones
flights_aug <-
augment(flights_fit, test_data)
flights_aug %>%
select(arr_delay, time_hour, flight, .pred_class, .pred_on_time)
flights_aug %>%
count(arr_delay) %>%
mutate(prop = n/sum(n))
# calcula metricas ----
metrics(flights_aug, arr_delay, .pred_class)
flights_aug %>%
roc_curve(truth = arr_delay, .pred_late) %>%
autoplot()
precision(flights_aug, arr_delay, .pred_class)
precision(flights_aug, arr_delay, .pred_class, estimator = "micro")
flights_aug %>%
accuracy(truth = arr_delay, .pred_class)
flights_aug %>%
roc_auc(truth = arr_delay, .pred_late)
flights_aug %>%
roc_auc(truth = arr_delay, .pred_on_time)
# validacion cruzada ----
folds <- vfold_cv(train_data, v = 2)
folds
flights_resample <-
flights_wflow %>%
fit_resamples(folds)
flights_resample
collect_metrics(flights_resample)
