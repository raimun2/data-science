}
}
models
for(m in models){}
siluetas <- NULL
for(m in models){
if(m == "hier"){
for(lnk in linkage){
for(dis in dists){
c <- clusteriza(data_tsne, model = m, linkage = lnk, h = dis)
siluetas <- rbind(siluetas, c(mean(Silhouette(c)[,3]), paste0(m, "-linkage:",lnk,"-h:",dis)))
}
}
}
if(m == "kmeans"){
for(ks in nks){
c <- clusteriza(data_tsne, model = m, k = ks)
siluetas <- rbind(siluetas, c(mean(Silhouette(c)[,3]), paste0(m, "-k:",ks)))
}
}
if(m == "cmeans"){
for(ks in nks){
for(fz in 2:10){
c <- clusteriza(data_tsne, model = m, k = ks, fuzz = fz)
siluetas <- rbind(siluetas, c(mean(Silhouette(c)[,3]), paste0(m, "-k:",ks,"-fuzz:",fz)))
}
}
}
if(m == "gmm"){
for(ks in nks){
for(cv in cov){
c <- clusteriza(data_tsne, model = m, k = ks, cov = cv)
siluetas <- rbind(siluetas, c(mean(Silhouette(c)[,3]), paste0(m, "-k:",ks,"-cov:",cv)))
}
}
}
if(m == "dbscan"){
for(ep in dists){
for(mp in 5*(1:9)){
c <- clusteriza(data_tsne, model = m, eps = ep, minpts = mp)
siluetas <- rbind(siluetas, c(mean(Silhouette(c)[,3]), paste0(m, "-eps:",eps,"-minpts:",mp)))
}
}
}
}
c
? Silhouette
siluetas <- rbind(siluetas, c(mean(Silhouette(c, d)[,3]), paste0(m, "-linkage:",lnk,"-h:",dis)))
Silhouette(c, d)
# ejecutamos un modelo para probar su clusterizacion
modelo_kmeans <- kmeans(data_tsne, 13)
# almacenamos los clusters en un vector que contiene el numero de cada cluster y el indice
clusters <- modelo_kmeans$cluster
siluetas <- rbind(siluetas, c(mean(silhouette(c, d)[,3]), paste0(m, "-linkage:",lnk,"-h:",dis)))
siluetas <- NULL
for(m in models){
if(m == "hier"){
for(lnk in linkage){
for(dis in dists){
c <- clusteriza(data_tsne, model = m, linkage = lnk, h = dis)
siluetas <- rbind(siluetas, c(mean(silhouette(c, d)[,3]), paste0(m, "-linkage:",lnk,"-h:",dis)))
}
}
}
if(m == "kmeans"){
for(ks in nks){
c <- clusteriza(data_tsne, model = m, k = ks)
siluetas <- rbind(siluetas, c(mean(silhouette(c, d)[,3]), paste0(m, "-k:",ks)))
}
}
if(m == "cmeans"){
for(ks in nks){
for(fz in 2:10){
c <- clusteriza(data_tsne, model = m, k = ks, fuzz = fz)
siluetas <- rbind(siluetas, c(mean(silhouette(c, d)[,3]), paste0(m, "-k:",ks,"-fuzz:",fz)))
}
}
}
if(m == "gmm"){
for(ks in nks){
for(cv in cov){
c <- clusteriza(data_tsne, model = m, k = ks, cov = cv)
siluetas <- rbind(siluetas, c(mean(silhouette(c, d)[,3]), paste0(m, "-k:",ks,"-cov:",cv)))
}
}
}
if(m == "dbscan"){
for(ep in dists){
for(mp in 5*(1:9)){
c <- clusteriza(data_tsne, model = m, eps = ep, minpts = mp)
siluetas <- rbind(siluetas, c(mean(silhouette(c, d)[,3]), paste0(m, "-eps:",eps,"-minpts:",mp)))
}
}
}
}
clusteriza <- function(data, model, k = NULL, linkage = "complete",
h = NULL, cov = "EII", minpts = 5,
eps = NULL, fuzz = 2, return.model = FALSE){
if(is.null(eps) & model == "dbscan") {eps <- data %>% cov() %>% diag() %>% sum() %>% sqrt()}
if(is.null(h) & model == "hier") {h <- data %>% cov() %>% diag() %>% sum() %>% sqrt()}
if(is.null(k) & (model %in% c("kmeans", "cmeans", "gmm"))) {k <- floor(sqrt(nrow(data)))}
if(model %in% models){
if(model == "kmeans"){
object   <- kmeans(data, k)
clusters <- object$cluster
} else if(model == "cmeans"){
object   <- e1071::cmeans(data, k, m = fuzz)
clusters <- object$cluster
} else if(model == "gmm"){
object   <- mclust::Mclust(data, G=k, modelNames = cov)
clusters <- object$classification
} else if(model == "dbscan"){
object   <- dbscan::dbscan(data, eps=eps, minPts  = minpts)
clusters <- object$cluster
} else if(model == "hier"){
object   <- hclust(dist(data), method = linkage)
clusters <- cutree(object, h = h)
}
if(return.model){
return(object)
} else {
return(clusters)
}
} else {
print("modelo no soportado")
}
}
siluetas <- NULL
for(m in models){
if(m == "hier"){
for(lnk in linkage){
for(dis in dists){
c <- clusteriza(data_tsne, model = m, linkage = lnk, h = dis)
siluetas <- rbind(siluetas, c(mean(silhouette(c, d)[,3]), paste0(m, "-linkage:",lnk,"-h:",dis)))
}
}
}
if(m == "kmeans"){
for(ks in nks){
c <- clusteriza(data_tsne, model = m, k = ks)
siluetas <- rbind(siluetas, c(mean(silhouette(c, d)[,3]), paste0(m, "-k:",ks)))
}
}
if(m == "cmeans"){
for(ks in nks){
for(fz in 2:10){
c <- clusteriza(data_tsne, model = m, k = ks, fuzz = fz)
siluetas <- rbind(siluetas, c(mean(silhouette(c, d)[,3]), paste0(m, "-k:",ks,"-fuzz:",fz)))
}
}
}
if(m == "gmm"){
for(ks in nks){
for(cv in cov){
c <- clusteriza(data_tsne, model = m, k = ks, cov = cv)
siluetas <- rbind(siluetas, c(mean(silhouette(c, d)[,3]), paste0(m, "-k:",ks,"-cov:",cv)))
}
}
}
if(m == "dbscan"){
for(ep in dists){
for(mp in 5*(1:9)){
c <- clusteriza(data_tsne, model = m, eps = ep, minpts = mp)
siluetas <- rbind(siluetas, c(mean(silhouette(c, d)[,3]), paste0(m, "-eps:",eps,"-minpts:",mp)))
}
}
}
}
m
h
lnk
dis
c <- clusteriza(data_tsne, model = m, linkage = lnk, h = dis)
silhouette(c, d)
c
? cutree
clusteriza <- function(data, model, k = NULL, linkage = "complete",
h = NULL, cov = "EII", minpts = 5,
eps = NULL, fuzz = 2, return.model = FALSE){
if(is.null(eps) & model == "dbscan") {eps <- data %>% cov() %>% diag() %>% sum() %>% sqrt()}
if(is.null(k) & (model %in% c("kmeans", "cmeans", "gmm"))) {k <- floor(sqrt(nrow(data)))}
if(model %in% models){
if(model == "kmeans"){
object   <- kmeans(data, k)
clusters <- object$cluster
} else if(model == "cmeans"){
object   <- e1071::cmeans(data, k, m = fuzz)
clusters <- object$cluster
} else if(model == "gmm"){
object   <- mclust::Mclust(data, G=k, modelNames = cov)
clusters <- object$classification
} else if(model == "dbscan"){
object   <- dbscan::dbscan(data, eps=eps, minPts  = minpts)
clusters <- object$cluster
} else if(model == "hier"){
object   <- hclust(dist(data), method = linkage)
clusters <- cutree(object, k = k)
}
if(return.model){
return(object)
} else {
return(clusters)
}
} else {
print("modelo no soportado")
}
}
for(m in models){
if(m == "hier"){
for(lnk in linkage){
for(ks in nks){
c <- clusteriza(data_tsne, model = m, linkage = lnk, k = ks)
siluetas <- rbind(siluetas, c(mean(silhouette(c, d)[,3]), paste0(m, "-linkage:",lnk,"-h:",dis)))
}
}
}
if(m == "kmeans"){
for(ks in nks){
c <- clusteriza(data_tsne, model = m, k = ks)
siluetas <- rbind(siluetas, c(mean(silhouette(c, d)[,3]), paste0(m, "-k:",ks)))
}
}
if(m == "cmeans"){
for(ks in nks){
for(fz in 2:10){
c <- clusteriza(data_tsne, model = m, k = ks, fuzz = fz)
siluetas <- rbind(siluetas, c(mean(silhouette(c, d)[,3]), paste0(m, "-k:",ks,"-fuzz:",fz)))
}
}
}
if(m == "gmm"){
for(ks in nks){
for(cv in cov){
c <- clusteriza(data_tsne, model = m, k = ks, cov = cv)
siluetas <- rbind(siluetas, c(mean(silhouette(c, d)[,3]), paste0(m, "-k:",ks,"-cov:",cv)))
}
}
}
if(m == "dbscan"){
for(ep in dists){
for(mp in 5*(1:9)){
c <- clusteriza(data_tsne, model = m, eps = ep, minpts = mp)
siluetas <- rbind(siluetas, c(mean(silhouette(c, d)[,3]), paste0(m, "-eps:",eps,"-minpts:",mp)))
}
}
}
}
# creamos vector vacio para almacenar siluetas
siluetas <- numeric(20)
# creamos vector vacio para almacenar siluetas
siluetas <- numeric(20)
for (k in 2:20){
# ejecutamos kmedias con k centroides
modelo <- kmeans(data_tsne, centers = k)
# cramos objeto con la silueta
temp <- silhouette(modelo$cluster, dist(data_tsne))
# almacenamos la silueta promedio del modelo
siluetas[k] <- mean(temp[,3])
}
tempDF <- data.frame(CS=siluetas, K=c(1:20))
# visualizamos
ggplot(tempDF, aes(x=K, y=CS)) +
geom_line() +
scale_x_continuous(breaks=c(1:20)) +
geom_vline(xintercept = which(tempDF$CS == max(tempDF$CS)), col = "red")
source("R/clusteriza.R")
# cargo librerias
pacman::p_load(tidyverse, Rtsne, mclust, e1071, cluster, flexclust, factoextra)
set.seed(42)
# cargo la data y aplico los mismos tratamientos que en el caso de DBScan
data_tsne  <- read.csv("data/video_games_sales.csv") %>%
mutate(User_Score = as.numeric(User_Score)) %>%
filter(!(is.na(Critic_Score) | is.na(User_Score))) %>%
select(Critic_Score, User_Score, User_Count, Global_Sales) %>%
unique() %>%
Rtsne() %>%
.$Y %>%
as.data.frame()
# exploramos graficamente la data
ggplot(data_tsne) +
geom_point(aes(V1,V2))
# antes de clusterizar, evaluamos el grado de clusterizacion natural de los datos de acuerdo
# al estadistico de hopkins, que esta implementado en la libreria factoextra.
#Calcula el hopkins statistic
(res <- get_clust_tendency(data_tsne, n = 30, graph = FALSE))
## Evaluacion de clusters
# ejecutamos un modelo para probar su clusterizacion
modelo_kmeans <- kmeans(data_tsne, 13)
# almacenamos los clusters en un vector que contiene el numero de cada cluster y el indice
clusters <- modelo_kmeans$cluster
# calculamos las distancias de los datos
distancias <- dist(data_tsne) %>% as.matrix()
View(distancias)
# generamos indices con la ubicacion de los clusters ordenados
clusters_i <-  sort(clusters, index.return=TRUE)
clusters_i$ix
#reordeno filas y columnas en base al cluster obtenido
distancias <- distancias[clusters_i$ix, clusters_i$ix]
rownames(distancias) <- c(1:nrow(data_tsne))
colnames(distancias) <- c(1:nrow(data_tsne))
# pero la matriz de distancias es muy grande para graficar
print(object.size(distancias), units = "Mb")
# la extraemos 1 de cada 10 filas y columnas
n <- 10
ids <- (1:floor(nrow(distancias)/n))*n
dist_reducida <- distancias[ids,ids]
# bajo considerablemente el tamaÃ±o
print(object.size(dist_reducida), units = "Mb")
# generamos la imagen de la matriz para la inspececion visual
image(dist_reducida)
# creo matriz llega de ceros
matriz_ideal <- matrix(0, nrow = nrow(data_tsne), ncol = nrow(data_tsne))
# creo matriz llega de ceros
matriz_ideal <- matrix(0, nrow = nrow(data_tsne), ncol = nrow(data_tsne))
# para cada cluster reemplazo con 1 en aquellas entidades que pertenezcan a el
for(k in unique(clusters_i$x)){
matriz_ideal[which(clusters_i$x==k), which(clusters_i$x==k)]  <- 1
}
# construyo matriz de disimilitud en base a la distancia ordenada
tempDist2 <- 1/(1+distancias)
? cor
# Calcula correlacion entre matriz de disimilitud y matriz optima
cor <- cor(matriz_ideal[upper.tri(matriz_ideal)],tempDist2[upper.tri(tempDist2)])
print(cor)
# creamos vector vacio para medir la cohesion en cada cluster
withinCluster <- numeric(length(unique(clusters)))
withinCluster
# creamos vector vacio para medir la cohesion en cada cluster
withinCluster <- numeric(length(unique(clusters)))
# para cada cluster
for (i in 1:length(withinCluster)){
# extraigo los puntos pertenecientes al cluster i
tempData <- data_tsne[which(clusters == i),]
# calculo la suma de distancias al cuadrado entre cada punto y el centroide
withinCluster[i] <- sum(dist2(tempData,colMeans(tempData))^2)
}
# calculo la suma total de cohesion para todos los clusters
cohesion <- sum(withinCluster)
#es equivalente a model$tot.withinss en k-means
print(c(cohesion, modelo_kmeans$tot.withinss))
centroide_total <- colMeans(data_tsne)
centroide_total
# creamos vector vacio para almacenar separacion en cada cluster
separation <-  numeric(length(unique(clusters)))
# para cada cluster
for (i in 1:length(separation)){
# extraigo los puntos pertenecientes al cluster i
tempData <- data_tsne[which(clusters==i),]
# calculo la separacion como la distancia promedio entre cada punto de un cluster y el resto
separation[i] <- nrow(tempData)*sum((meanData-colMeans(tempData))^2)
}
# creamos vector vacio para almacenar separacion en cada cluster
separation <-  numeric(length(unique(clusters)))
# para cada cluster
for (i in 1:length(separation)){
# extraigo los puntos pertenecientes al cluster i
tempData <- data_tsne[which(clusters==i),]
# calculo la separacion como la distancia promedio entre cada punto de un cluster y el resto
separation[i] <- nrow(tempData)*sum((separation-colMeans(tempData))^2)
}
(sum(separation))
centroide_total <- colMeans(data_tsne)
# creamos vector vacio para almacenar separacion en cada cluster
separation <-  numeric(length(unique(clusters)))
# para cada cluster
for (i in 1:length(separation)){
# extraigo los puntos pertenecientes al cluster i
tempData <- data_tsne[which(clusters==i),]
# calculo la separacion como la distancia promedio entre cada punto de un cluster y el resto
separation[i] <- nrow(tempData)*sum((centroide_total-colMeans(tempData))^2)
}
(sum(separation))
? silhouette
clusters
# el coeficiente recibe las etiquetas de cluster y la matriz de distancias de la data original
coefSil <- silhouette(clusters, dist(data_tsne))
summary(coefSil)
coefSil[,3]
#visualizamos el codigo de silueta de cada cluster
fviz_silhouette(coefSil) + coord_flip()
# el coeficiente recibe las etiquetas de cluster y la matriz de distancias de la data original
coefSil <- silhouette(clusters, dist(data_tsne))
#visualizamos el codigo de silueta de cada cluster
fviz_silhouette(coefSil) + coord_flip()
# creamos vector vacio para almacenar siluetas
siluetas <- numeric(20)
# creamos vector vacio para almacenar siluetas
siluetas <- numeric(20)
for (k in 2:20){
# ejecutamos kmedias con k centroides
modelo <- kmeans(data_tsne, centers = k)
# cramos objeto con la silueta
temp <- silhouette(modelo$cluster, dist(data_tsne))
# almacenamos la silueta promedio del modelo
siluetas[k] <- mean(temp[,3])
}
tempDF <- data.frame(CS=siluetas, K=c(1:20))
# visualizamos
ggplot(tempDF, aes(x=K, y=CS)) +
geom_line() +
scale_x_continuous(breaks=c(1:20)) +
geom_vline(xintercept = which(tempDF$CS == max(tempDF$CS)), col = "red")
source("R/clusteriza.R")
tuesdata <- tidytuesdayR::tt_load('2020-09-22')
library(munro)
pacman::p_load(tidymodels)
library(rlang)
library(tidymodels)
tidymodels_prefer()
pacman::p_load(tidymodels)
tidymodels_prefer()
data(Chicago)
n <- nrow(Chicago)
Chicago <- Chicago %>% select(ridership, Clark_Lake, Quincy_Wells)
Chicago_train <- Chicago[1:(n - 7), ]
Chicago_test <- Chicago[(n - 6):n, ]
knn_reg_spec <-
nearest_neighbor(neighbors = 5, weight_func = "triangular") %>%
# This model can be used for classification or regression, so set mode
set_mode("regression") %>%
set_engine("kknn")
knn_reg_spec
knn_reg_fit <- knn_reg_spec %>% fit(ridership ~ ., data = Chicago_train)
knn_reg_fit
install.packages("kknn")
knn_reg_fit <- knn_reg_spec %>% fit(ridership ~ ., data = Chicago_train)
knn_reg_fit
predict(knn_reg_fit, Chicago_test)
data(two_class_dat)
data_train <- two_class_dat[-(1:10), ]
data_test  <- two_class_dat[  1:10 , ]
knn_cls_spec <-
nearest_neighbor(neighbors = 11, weight_func = "triangular") %>%
# This model can be used for classification or regression, so set mode
set_mode("classification") %>%
set_engine("kknn")
knn_cls_spec
knn_cls_fit <- knn_cls_spec %>% fit(Class ~ ., data = data_train)
knn_cls_fit
bind_cols(
predict(knn_cls_fit, data_test),
predict(knn_cls_fit, data_test, type = "prob")
)
? tidymodels_prefer
View(Chicago)
# modelo prediccion
# cargamos la data
data(Chicago)
View(Chicago)
? Chicago
# seleccionamos variables relevantes
Chicago <- Chicago %>% select(ridership, Clark_Lake, Quincy_Wells)
Chicago_train <- Chicago[1:(n - 7), ]
n
1:(n - 7)
(n - 6):n
# cargo la data
data(two_class_dat)
? two_class_dat
View(two_class_dat)
# cargo librerias
pacman::p_load(tidymodels, kknn)
# modelo prediccion
# cargamos la data
data(Chicago)
View(Chicago)
# identificamos el numero de filas
n <- nrow(Chicago)
# seleccionamos variables relevantes
Chicago <- Chicago %>% select(ridership, Clark_Lake, Quincy_Wells)
# separo datos de entrenamiento y de prueba
Chicago_train <- Chicago[1:(n - 7), ]
Chicago_test <- Chicago[(n - 6):n, ]
View(Chicago_test)
# construyo el modelo
knn_reg_spec <-
nearest_neighbor(neighbors = 5, weight_func = "triangular") %>%
set_mode("regression") %>%
set_engine("kknn")
# veo el modelo
knn_reg_spec
# ajusto parametros del modelo
knn_reg_fit <- knn_reg_spec %>% fit(ridership ~ ., data = Chicago_train)
# veo prediccion
knn_reg_fit
# predigo valores para conjunto de prueba
predict(knn_reg_fit, Chicago_test)
# cargo la data
data(two_class_dat)
View(two_class_dat)
# separo datos de entrenamiento y de prueba
data_train <- two_class_dat[-(1:10), ]
data_test  <- two_class_dat[  1:10 , ]
knn_cls_spec <-
nearest_neighbor(neighbors = 11, weight_func = "triangular") %>%
set_mode("classification") %>%
set_engine("kknn")
knn_cls_spec
knn_cls_fit <- knn_cls_spec %>% fit(Class ~ ., data = data_train)
knn_cls_fit
bind_cols(
predict(knn_cls_fit, data_test),
predict(knn_cls_fit, data_test, type = "prob")
)
bind_cols(
predict(knn_cls_fit, data_test),
predict(knn_cls_fit, data_test)
)
bind_cols(
predict(knn_cls_fit, data_test),
predict(knn_cls_fit, data_test, type = "prob")
)
